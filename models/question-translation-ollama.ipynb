{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain chromadb pypdf\n",
    "%pip install chromadb\n",
    "%pip install langchain-text-splitters\n",
    "%pip install langchain-core\n",
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50f389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54a2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question                 | Difficulty Level\n",
      "---------------------------------------------------------------------\n",
      "What is a class in Java? | 1\n",
      "How do you define a method in Java? | 1\n",
      "What is the purpose of the main method in Java? | 1\n",
      "What is a variable in Java? | 1\n",
      "How do you declare an array in Java? | 1\n",
      "What is the difference between int and float in Java? | 1\n",
      "What is the syntax for a for loop in Java? | 1\n",
      "How do you create an object in Java? | 1\n",
      "What is a constructor in Java? | 1\n",
      "What is the\n"
     ]
    }
   ],
   "source": [
    "# local_path = '../docs/240-core-java-interview-questions-and-answers.pdf'\n",
    "\n",
    "# # Local PDF file uploads\n",
    "# pdf_loader = PyPDFLoader(local_path)\n",
    "# documents = pdf_loader.load()\n",
    "# documents[0]\n",
    "\n",
    "# Specify the local path to your text file\n",
    "local_path = '../docs/java.txt'\n",
    "\n",
    "# Load the text file\n",
    "with open(local_path, 'r', encoding='utf-8') as file:\n",
    "    text_content = file.read()\n",
    "\n",
    "# Display the first few lines or the entire content\n",
    "print(text_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76364295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1f841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED      \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    3 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdab411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "decfcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Document class\n",
    "class Document:\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.page_content = content\n",
    "        self.metadata = metadata if metadata is not None else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4f30176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents([Document(text_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29fcb154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 13/13 [00:37<00:00,  2.91s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"microservice-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf6d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b632e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783e24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5968cefd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m retriever \u001b[38;5;241m=\u001b[39m MultiQueryRetriever\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mvector_db\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever(), \n\u001b[0;32m      3\u001b[0m     llm,\n\u001b[0;32m      4\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mQUERY_PROMPT\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# RAG prompt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnswer the question based ONLY on the following context:\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_db' is not defined"
     ]
    }
   ],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16403897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
